"""Dependency Inversion: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞ –Ω–∞ CLIP"""
import os
import json
import threading
import re
import numpy as np
import torch
from typing import List, Tuple, Optional
from sentence_transformers import SentenceTransformer, util
from domain.interfaces import ISearchEngine, IFrameRepository
from domain.entities import VisualFrame


class ClipSearchEngine(ISearchEngine):
    """Single Responsibility: –ü–æ–∏—Å–∫ –∫–∞–¥—Ä–æ–≤ –ø–æ —Ç–µ–∫—Å—Ç—É —Å –ø–æ–º–æ—â—å—é CLIP"""
    
    def __init__(
        self, 
        repository: IFrameRepository,
        model_name: str = 'clip-ViT-B-32-multilingual-v1',
        cache_file: str = "data/visual_db.npy",
        feedback_file: str = "data/feedback.json"
    ):
        self.repository = repository
        self.model_name = model_name
        self.cache_file = cache_file
        self.feedback_file = feedback_file
        self.model: Optional[SentenceTransformer] = None
        self.frames: List[VisualFrame] = []
        self.embeddings: Optional[torch.Tensor] = None
        self.feedback_lock = threading.Lock()
        self.feedback = {"positive": set(), "negative": set()}
        self._weights = {"text": 0.7, "tags": 0.3}
        self._stop_words = {
            "–∏", "–≤", "–Ω–∞", "—Å", "–ø–æ", "–∫", "–æ", "–∑–∞", "–¥–ª—è", "–∫–∞–∫", "—á—Ç–æ", "—ç—Ç–æ", "–∏–∑", "–∏–ª–∏", "–Ω–æ",
            "the", "and", "for", "with", "about", "from", "into", "over", "under", "been", "were",
            "was", "are", "you", "your", "our", "–º—ã", "–æ–Ω–∏", "–æ–Ω–∞", "–æ–Ω", "–µ–≥–æ", "–µ–µ", "–∏—Ö", "—Ç–∞–º",
            "then", "than", "that", "this", "those", "these", "–ø–æ—Ç–æ–º", "—Ç–æ–≥–¥–∞", "–µ—â–µ", "–µ—â—ë",
        }
        self._initialized = False
    
    def _initialize(self) -> None:
        """Lazy initialization: –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏"""
        if self._initialized:
            return
        
        print("üß† –ó–∞–≥—Ä—É–∂–∞—é –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—É—é –ù–µ–π—Ä–æ—Å–µ—Ç—å (Multilingual CLIP)...")
        self.model = SentenceTransformer(self.model_name)
        self.frames = self.repository.load_all()
        self._load_feedback()
        self._load_or_index_images()
        self._initialized = True
    
    def is_ready(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –¥–≤–∏–∂–∫–∞"""
        if not self._initialized:
            self._initialize()
        return self.embeddings is not None and len(self.frames) > 0
    
    def search(self, query_text: str, limit: int = 5) -> List[Tuple[VisualFrame, float]]:
        """–ò—â–µ—Ç –∫–∞–¥—Ä—ã –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É"""
        if not self.is_ready():
            return []
        
        aggregated_hits = {}
        query_text = query_text.strip()
        
        tag_query = self._extract_tags_internal(query_text)
        
        # –ü–æ–∏—Å–∫ –ø–æ –ø–æ–ª–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É
        if query_text:
            text_emb = self.model.encode(query_text, convert_to_tensor=True)
            text_hits = util.semantic_search(
                text_emb, self.embeddings, top_k=max(limit * 3, 15)
            )
            self._merge_hits(text_hits, "text", aggregated_hits)
        
        # –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–≥–∞–º
        if tag_query:
            tag_emb = self.model.encode(tag_query, convert_to_tensor=True)
            tag_hits = util.semantic_search(
                tag_emb, self.embeddings, top_k=max(limit * 2, 10)
            )
            self._merge_hits(tag_hits, "tags", aggregated_hits)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = []
        for data in aggregated_hits.values():
            scores = data["scores"]
            combined = sum(
                self._weights.get(source, 0.0) * score 
                for source, score in scores.items()
            )
            if "text" not in scores:
                combined *= 0.8  # –®—Ç—Ä–∞—Ñ –∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            results.append((data["frame"], combined))
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
        adjusted = []
        for frame, score in results:
            key = self._feedback_key(frame)
            if key in self.feedback["negative"]:
                score *= 0.2
            elif key in self.feedback["positive"]:
                score *= 1.25


            final_score = max(0.0, min(score, 1.0))


            adjusted.append((frame, final_score))
        
        adjusted.sort(key=lambda item: item[1], reverse=True)
        return adjusted[:limit]
    
    def record_feedback(self, frame: VisualFrame, is_positive: bool) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å"""
        if frame is None:
            return
        
        key = self._feedback_key(frame)
        with self.feedback_lock:
            if is_positive:
                self.feedback["negative"].discard(key)
                self.feedback["positive"].add(key)
            else:
                self.feedback["positive"].discard(key)
                self.feedback["negative"].add(key)
            self._persist_feedback()
    
    def extract_tags(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–ø—É–±–ª–∏—á–Ω—ã–π –º–µ—Ç–æ–¥ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞)"""
        tags_str = self._extract_tags_internal(text)
        return [tag.strip() for tag in tags_str.split(',') if tag.strip()]
    
    def _extract_tags_internal(self, text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        tokens = re.findall(r"[A-Za-z–ê-–Ø–∞-—è—ë–Å0-9]+", text.lower())
        keywords = []
        seen = set()
        
        for token in tokens:
            if len(token) < 4 or token in self._stop_words:
                continue
            if token not in seen:
                seen.add(token)
                keywords.append(token)
            if len(keywords) >= 12:
                break
        
        # –ë–∏–≥—Ä–∞–º–º—ã
        bigrams = []
        for i in range(len(tokens) - 1):
            a, b = tokens[i], tokens[i + 1]
            if a in self._stop_words or b in self._stop_words:
                continue
            phrase = f"{a} {b}"
            if len(phrase) >= 8:
                bigrams.append(phrase)
            if len(bigrams) >= 4:
                break
        
        combined = keywords + bigrams
        return ", ".join(combined[:15])
    
    def _encode_text(self, text: str) -> torch.Tensor:
        """–ö–æ–¥–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥"""
        return self.model.encode(text, convert_to_tensor=True)
    
    def _merge_hits(self, hits, source_label: str, storage: dict) -> None:
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞"""
        for hit in hits[0]:
            idx = hit["corpus_id"]
            entry = storage.setdefault(idx, {"scores": {}, "frame": self.frames[idx]})
            score = float(hit["score"])
            entry["scores"][source_label] = max(entry["scores"].get(source_label, 0.0), score)
    
    def _feedback_key(self, frame: VisualFrame) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"""
        ts = round(frame.timestamp, 2)
        return f"{frame.video_filename}|{ts}"
    
    def _load_feedback(self) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –∏–∑ —Ñ–∞–π–ª–∞"""
        if not os.path.exists(self.feedback_file):
            return
        
        try:
            with open(self.feedback_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            self.feedback["positive"] = set(data.get("positive", []))
            self.feedback["negative"] = set(data.get("negative", []))
        except Exception:
            pass
    
    def _persist_feedback(self) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –≤ —Ñ–∞–π–ª"""
        os.makedirs(os.path.dirname(self.feedback_file), exist_ok=True)
        data = {
            "positive": sorted(self.feedback["positive"]),
            "negative": sorted(self.feedback["negative"]),
        }
        with open(self.feedback_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def _load_or_index_images(self) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∫–∞–¥—Ä–æ–≤"""
        if not self.frames:
            return
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        if os.path.exists(self.cache_file) and len(self.frames) > 0:
            try:
                cached_emb = np.load(self.cache_file)
                if len(cached_emb) == len(self.frames):
                    self.embeddings = torch.from_numpy(cached_emb)
                    print(f"‚ö°Ô∏è –ö—ç—à –≤–µ–∫—Ç–æ—Ä–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω ({len(self.embeddings)} —à—Ç).")
                    return
                else:
                    os.remove(self.cache_file)
            except Exception:
                pass
        
        self._run_full_indexing()
    
    def _run_full_indexing(self) -> None:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –∫–∞–¥—Ä–æ–≤"""
        print(f"üìä –ò–Ω–¥–µ–∫—Å–∏—Ä—É—é {len(self.frames)} –∫–ª—é—á–µ–≤—ã—Ö –∫–∞–¥—Ä–æ–≤...")
        image_paths = []
        valid_frames = []
        
        for frame in self.frames:
            if os.path.exists(frame.frame_path):
                image_paths.append(frame.frame_path)
                valid_frames.append(frame)
        
        self.frames = valid_frames
        if image_paths:
            print(f"üöÄ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(image_paths)} —Ñ–∞–π–ª–æ–≤...")
            self.embeddings = self.model.encode(
                image_paths, 
                batch_size=32, 
                convert_to_tensor=True, 
                show_progress_bar=True
            )
            np.save(self.cache_file, self.embeddings.cpu().numpy())
            print("‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –∫—ç—à.")
        else:
            print("‚ùå –û—à–∏–±–∫–∞: –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")

